{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meyush0/Email-Campaign-Effectiveness-Prediction/blob/main/Email_Campaign_Effectiveness_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name** -  **Email Campaign Effectiveness Prediction**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Ayush Singh( Individual )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "Email campaign is a sequence of marketing efforts that contacts multiple recipients at once. Email campaigns are designed to reach out to subscribers at the best time and provide valuable content and relevant offers. Using email campaigns allows businesses to build deep and trusting relationships with their customers. Marketing through Email can make communication with clients easier and more effective. Email campaigns are a very powerful medium between a business company and its audience. It helps not only to increase sales but build brand image. Most of the small to medium business owners are making effective use of Gmail-based Email marketing Strategies for offline targeting of converting their prospective customers into leads so that they stay with them in business.\n",
        "\n",
        "The main objective is to create a machine learning model to characterize the mail and track the mail that is ignored; read; acknowledged by the reader.\n",
        "\n",
        "Performing exploratory data analysis helped us to understand the features and relationships that they have and their impact on the target or the client's response and find out important insights.\n",
        "\n",
        "Data is labeled and the target column being categorical, I implemented classification based machine learning algorithms to complete the prediction task.\n",
        "\n",
        "The Email campaign data contains various types of information regarding the emails that were sent, it contains information about their customers and their responses. Checking the shape of the data, I found that it has 68353 observations and 12 columns.\n",
        "\n",
        "Null values and outliers were treated accordingly. I checked data distributions for various features. New features were created from existing and correlated features to solve the problem of multicollinearity. I used Synthetic Minority Oversampling Technique (SMOTE) to handle the imbalance in the target column. I used the model such as Logistic Regression, Random Forest classifier and Xgboost classifier and also tuned it with Hyperparameter. To evaluate the performance of the model, I split our data into a training set and a testing set. I used the training set to fit the model and the testing set to evaluate its performance. I used a variety of metrics, such as precision, recall, and F1 score, to assess the model's accuracy and effectiveness, but the problem statement clearly mentioned that we need to characterize the mails based on the user response, thus I decided to use F1 Score and AUC-ROC Score then I compared these evaluation metrics of each classifiers and found the best model among all model. After that I checked the feature importance for the model that performed the best.\n",
        "\n",
        "Once got the best model, it can be deployed in a production environment to help small to medium business owners improve the effectiveness of their email marketing campaigns. By using the model to characterize and track emails, they will be able to make more informed decisions about how to target their marketing efforts and increase customer retention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "https://github.com/meyush0/Email-Campaign-Effectiveness-Prediction/tree/main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "Small to medium business owners are using Gmail-based email marketing strategies to convert prospective customers into leads, but they are unable to track which emails are being ignored, read, or acknowledged by the reader. They want to create a machine learning model to help characterize and track these emails. The main objective is to improve the effectiveness of their email marketing efforts and increase customer retention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4qUhDuk7pgYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/ML - Capstone ( Classification)/data_email_campaign.csv\")"
      ],
      "metadata": {
        "id": "p36IpTG-pRAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First 5 Look\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m2jE6dYoOuR"
      },
      "outputs": [],
      "source": [
        "#last 5 values\n",
        "data.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "print(f\"There are {data.shape[0]} rows and {data.shape[1]} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "print(f\"There are {len(data[data.duplicated()])} duplicates in the dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(data.isnull(),cbar=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "I got to know the following things about the dataset:\n",
        "* There are 68353 rows and 12 columns present in the dataset.\n",
        "* Information about the Datatype of each column.\n",
        "* There are null values in four features namely, customer_location, total_past_communication, Total_links, Total_images.\n",
        "* There are no duplicate values found!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "#column names\n",
        "data.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAB4Uf-1oOuV"
      },
      "outputs": [],
      "source": [
        "#new dataframe\n",
        "Cat_data=data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ar79f4XoOuV"
      },
      "outputs": [],
      "source": [
        "#change datatype of ['Email_Type','Email_Source_Type','Email_Campaign_Type','Time_Email_sent_Category','Email_Status'] from int to object\n",
        "#because it is also a nominal categorial data and we want their categorical description.\n",
        "Cat_data[['Email_Type','Email_Source_Type','Email_Campaign_Type','Time_Email_sent_Category','Email_Status']]=Cat_data[['Email_Type',\n",
        "                                          'Email_Source_Type','Email_Campaign_Type','Time_Email_sent_Category','Email_Status']].astype(\"str\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# categorical Description\n",
        "Cat_data.describe(include=object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "* There are two unique email type 1 and 2, where 1 is on the top with frequency 48866.\n",
        "\n",
        "* Email_Source_Type has 2 unique values 1 and 2, where 1 is on the top with frequency 37149.\n",
        "\n",
        "* There is 7 different demographic location of the customers. Most of the customers is in G location with frequency 23173.\n",
        "\n",
        "* Email_Campaign_Type has 3 unique campaign type of the email which are 1,2 and 3, where 2 is mostly used with frequency 48273.\n",
        "\n",
        "* Time_Email_sent_Category has 3 categories 1,2 and 3 in which 2 is most prefer time of sending the email.\n",
        "\n",
        "* Email_Status also has 3 categories 0,1 and 2 in which mostly the status of the email is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri6WKB-hoOuX"
      },
      "outputs": [],
      "source": [
        "#numerical description\n",
        "Cat_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73BWJv3hoOuX"
      },
      "source": [
        "### **Observations:**\n",
        "\n",
        "* Subject Hotness Score is float value ranges from 0 to 5, where 0 is not hot, 5 is very hot. The average subject hotness score for the given data set is around 1.10\n",
        "\n",
        "* Average total past communications is around 29. maximum total past communications are around 67 and the minimum is 0.\n",
        "\n",
        "* Average word count is around 700 words. An email was sent with maximum words of around 1316 words. An email was sent with minimum words around 40 words.\n",
        "\n",
        "* Average total links sent is around 10 links, and the maximum total links sent is around 49 links, the minimum is 1 link.\n",
        "\n",
        "* Average total images sent through an email is around 4 images. The maximum total images sent are around 45 images and the minimum is 0.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiMcuTN-oOuY"
      },
      "source": [
        "# Variables Description\n",
        "\n",
        "* **Email_Id** - Email id of customer\n",
        "* **Email_Type** - Email type contains 2 categories 1 and 2. We can assume that the types are like promotional email or sales email.\n",
        "* **Subject_Hotness_Score** - It is the email's subject's score on the basis of how good and effective the content is.\n",
        "* **Email_Source_Type** - It represents the source of the email like sales,marketing or product type email.\n",
        "* **Email_Campaign_Type** - The campaign type of the email.\n",
        "* **Customer_Location** - Categorical data which explains the different demographic location of the customers.\n",
        "* **Total_Past_Communications** - This columns contains the total previous mails from the same source.\n",
        "* **Time_Email_sent_Category** - It has 3 categories: 1,2 and 3 which are considered as morning,evening and night time slot.\n",
        "* **Word_Count** - Total count of word in each email\n",
        "* **Total_links** - Total number of links in the email\n",
        "* **Total_Images** - Total Number of images in the email\n",
        "* **Email_Status** - Our target variable which contains whether the mail was ignored, read, acknowledged by the reader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in data.columns.tolist():\n",
        "  print(\"No. of unique values in '{}' is {}.\".format(i, data[i].nunique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***\n",
        "\n",
        "Data Wrangling is the process of gathering, collecting, and transforming Raw data into another format for better understanding, decision-making, accessing, and analysis in less time. Data Wrangling is also known as Data Munging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "outputs": [],
      "source": [
        "data.drop(columns=\"Email_ID\",inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "## extracting the numerical features\n",
        "\n",
        "num_feature=Cat_data.describe().columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQjdLXpeoOui"
      },
      "outputs": [],
      "source": [
        "## creating a set of categorical features excluding 'Email_Status' and 'Email_ID'.\n",
        "cat_feature=set(Cat_data.describe(include=\"object\").columns.values)-{'Email_Status','Email_ID'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "manGp4FCoOun"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dnrj2bMCoOun"
      },
      "outputs": [],
      "source": [
        "table =[{value:pd.pivot_table(data, values =value, index =['Email_Type', 'Email_Source_Type','Time_Email_sent_Category'],\n",
        "                         columns =['Email_Status'], aggfunc = np.sum)} for value in num_feature]\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "From the above manipulations, for the columns of Email_Status 0, 1 and 2 respectively, I found out that\n",
        "\n",
        "* Subject_Hotness_Score is maximum for Email_Type 1, for Email_Source_type 2 and for Time_Email_Sent_Category 2.\n",
        "\n",
        "* Total_Past_Communications is maximum for Email_Type 1, for Email_Source_type 1 and for Time_Email_Sent_Category 2.\n",
        "\n",
        "* Email_Status is maximum for Email_Type 1, for Email_Source_type 2 and for Time_Email_Sent_Category 2.\n",
        "\n",
        "* Total_Links is maximum for Email_Type 1, for Email_Source_type 2 and for Time_Email_Sent_Category 2.\n",
        "\n",
        "* Total_Images is maximum for Email_Type 1, for Email_Source_type 2 and for Time_Email_Sent_Category 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1  Discrete Distribution of Target Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# visualize the target variable\n",
        "data2 = data['Email_Status'].value_counts()\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot pie chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.pie(data2, autopct='%1.2f%%', labels=data2.index, colors=('lightpink', 'yellow', 'thistle'))\n",
        "plt.title('Email_Status Distribution', fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "Since our target variable is categorical, and we know that Bar Chart and Pie Chart are typically used to visualize categorical data.\n",
        "\n",
        "* A **bar chart** places the separate values of the data on the x-axis and the height of the bar indicates the count of that category.\n",
        "* A **pie plot** is a proportional representation of the numerical data in a column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        " From the above Bar chart and Pie chart, we conclude that\n",
        " * No. of Email Read :- 11039 i.e., 16.15%\n",
        " * No. of Email Acknowledged :- 2373 i.e., 3.47%\n",
        " * No. of Email Ignored :- 54941 i.e., 80.38%\n",
        "\n",
        " This result shows that most of the emails were ignored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "The gained insights will help us in understanding the effectiveness of email campaign. As we saw that No. of Email Acknowledged are only 3.47%, where as No. of Email Read and Ignored are 16.15% and 80.38% respectively.\n",
        "for this, we find the factor which make the email acknowledged and use it in other email too, so that it help in the growth of business.\n",
        "\n",
        "Negative growth means decrement of no. of email acknowledged, since  No. of Email Acknowledged is related less as compare to the other email. if it is decreasing then it lead to negative growth in the business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2 Distribution of Numerical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# visualization of numerical feature\n",
        "from scipy import stats\n",
        "\n",
        "for col in num_feature :\n",
        "    # sns.set_style(\"ticks\")\n",
        "    # sns.set_context(\"poster\");\n",
        "    plt.figure(figsize=(25,6))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    fig = sns.boxplot(y=data[col], color='#00FF7F')\n",
        "    fig.set_ylabel(col)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    sns.distplot(data[col], color = '#055E85', fit = stats.norm);\n",
        "    feature = data[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n",
        "    plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right')\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    f=sns.boxplot(x=data[\"Email_Status\"],y=data[col])\n",
        "    f.set_xticklabels(['Ignored','Read', 'Acknowledged'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "Distplot and Boxplot are best for plotting continuous variable and understanding the distribution of the data and visualizing outliers as well as quartiles positions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "***Boxplot***\n",
        "---\n",
        "Boxplot gives us 5 point summary which consists of the minimum point, the first quartile, the median, the third quartile, and the maximum point.\n",
        "\n",
        "from Boxplot, we have got to know about outliers, as we see that, all numerical variable have outliers except wordcount.\n",
        "\n",
        "---\n",
        "***Distplot***\n",
        "---\n",
        "As we know that, positive skewed, negative skewed and no skewed in the data is determined by mean, median amd mode.\n",
        "\n",
        "if mean > median > mode then, distribution of the data is positively skewed,\n",
        "\n",
        "if mean = median = mode then, no skewed that is normally distributed,\n",
        "\n",
        "otherwise, it is negatively skewed.\n",
        "\n",
        "Subject hotness score, total image and total links are positively skewed whereas wordcount and total comunication count show somewhat normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "These plot were drawn to understand distribution of each variable which eventually will help in building model and treating with null values or outliers, however it helps how each values plays important role in creating an effective email campaign.\n",
        "\n",
        "There are no such insights that lead to negative growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3  Distribution of Categorical Variables and Categorical Features vs Email Status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "categorical_variables = ['Email_Type','Email_Source_Type','Customer_Location','Email_Campaign_Type','Time_Email_sent_Category']\n",
        "Target_var = ['Email_Status']\n",
        "\n",
        "for i,value in enumerate(categorical_variables):\n",
        "  ax = sns.countplot(x=data[value], hue=data[Target_var[0]])\n",
        "  unique = len([x for x in data[value].unique() if x==x])\n",
        "  bars = ax.patches\n",
        "  for i in range(unique):\n",
        "      catbars=bars[i:][::unique]\n",
        "      #get height\n",
        "      total = sum([x.get_height() for x in catbars])\n",
        "      #print percentage\n",
        "      for bar in catbars:\n",
        "        ax.text(bar.get_x()+bar.get_width()/2.,\n",
        "                    bar.get_height(),\n",
        "                    f'{bar.get_height()/total:.0%}',\n",
        "                    ha=\"center\",va=\"bottom\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "Countplot is best for plotting Categorical variable, it is used to represent the occurrence(counts) of the observation present in the categorical variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "As we can observe that\n",
        "* The percentage of each class in each categorical variable.\n",
        "* The distribution of Email_Status is almost similar in all the categories except in Email_Campaign_Type we can see that it shows a totally different trend.\n",
        "For Email_Campaign_Type=1 we see that no. of email ignored < no. of email acknowledged < no. of email Read"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "The gained insights will help us in understanding the effectiveness of email campaign. Since campaign type 1 show more engagement if Company increases campaign type 1 then company moves in the direction of Positive growth.\n",
        "\n",
        "if company increases campaign type 2 then company moves in the direction of negative growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4 Correlation Heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPZVcRNIoOu5"
      },
      "source": [
        "#### Spearman Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "sns.set_context('notebook')\n",
        "plt.figure(figsize = (14,8))\n",
        "plt.xticks(fontsize= 14)\n",
        "plt.yticks(fontsize= 14)\n",
        "sns.heatmap(data[num_feature].corr(), annot=True,linewidth=.5,cmap=\"Greens\")\n",
        "plt.title(\"Pearson Correlation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "A correlation heatmap is a graphical representation of a correlation matrix representing the correlation between different variables. Each cell shows the correlation between two variables. The value of correlation can take any value from -1 to 1.\n",
        "\n",
        "The Pearson correlation coefficient is used to measure the strength of a linear association between two numerical variables\n",
        "\n",
        "Thus to know the correlation between all the numerical variables along with the correlation coeficients, i used Pearson correlation heatmap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "By using Pearson correlation, we observed that There is a high positive correlation (i.e., 0.78 ) between Total links and Total image which causes multicollinearity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Spearman Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "plt.figure(figsize = (14,8))\n",
        "plt.xticks(fontsize= 14)\n",
        "plt.yticks(fontsize= 14)\n",
        "heatmap= sns.heatmap(data.corr(method=\"spearman\"),vmin= -1,vmax=1,annot=True)\n",
        "plt.title(\"Spearman Correlation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "Correlation heatmaps can be used to find potential relationships between variables and to understand the strength of these relationships. The value of correlation can take any value from -1 to 1.\n",
        "\n",
        "Spearman rank correlation is a non-parametric test that is used to measure the degree of association between two variables.\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, i used Spearman correlation heatmap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "By using Spearman correlation Heatmap, we observed that\n",
        "* There is a high positive correlation (i.e., 0.64 ) between Total links and Total image.\n",
        "* There is a high negative correlation (i.e., -0.68 ) between Email_Campaign_Type and Subject_hotness_score.\n",
        "* As compare to all other variable, Total_Past_Communications has higher correlation with target variable (Email_Status)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6 Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data,hue =\"Email_Status\", diag_kind = \"kde\" ,kind = \"scatter\",palette = \"rocket\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "Pair plot allows us to look at the diagonal distribution of these variable and on the non-diagonal linear relationship between the variables.\n",
        "\n",
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and relationship between the features. It's exactly same as the correlation heatmap but here you will get the graphical representation of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "From the above chart I got to know, there are less linear relationship between variables.\n",
        "\n",
        "Total links and total image show some linear relation and we already know they are correlated as seen in earlier heatmap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1\n",
        "----\n",
        "Test whether a Total_Past_Communications has a Normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "\n",
        "* Null Hypothesis H0: Total_Past_Communications has a normal distribution.\n",
        "\n",
        "* Alternative Hypothesis H1: Total_Past_Communications does not have a normal distribution.\n",
        "* Test Type : Shapiro-Wilk Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# perform Shapiro-Wilk Normality Test\n",
        "from scipy.stats import shapiro\n",
        "stat, p = shapiro(data[\"Total_Past_Communications\"])\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "    print('Null hypothesis is probably true, i.e. Total_Past_Communications has a normal distribution')\n",
        "else:\n",
        "    print('Null hypothesis is probably false, i.e Total_Past_Communications does not have a normal distribution.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "I used Shapiro-Wilk Test to obtain P-Value, to check whether a data has a Normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "Shapiro-Wilk Test is the appropriate test for testing the normality of data. I used this test on Total_Past_Communications variable because it has the highest correlation with target variable (Email_Status) among all the numerical variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2\n",
        "---\n",
        "The Email_Type of the campaign will not have any significant impact on the Email_Status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "* Null Hypothesis: There is no relationship between Email_Type and Email_Status.\n",
        "* Alternative Hypothesis: There is a relationship between Email_Type and Email_Status.\n",
        "* Test Type : chi-square test of independence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# perform chi-square test of independence\n",
        "chi2, p_value, dof, expected = stats.chi2_contingency(pd.crosstab(data['Email_Type'], data['Email_Status']))\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis - the Email_Type has a significant impact on the Email_Status\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis - the Email_Type does not have a significant impact on the Email_Status\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "For these hypothesis, I used chi-square test of independence which is a statistical test to determine whether there is a significant association between two categorical variables. In this case, the two variables are Email_Type and Email_Status."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "This test is appropriate for the determination of existence of any relationship between the two categorical variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3\n",
        "----\n",
        "The Customer_Location will not have any significant impact on the Total_Links, Total_Images and Total_Past_Communications in the email"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "* Null Hypothesis: The mean of Total_Links is equal among the location (A, B, C, D, E, F, G) (H0: μ1 = μ2 = μ3 = μ4 = μ5 = μ6 = μ7)\n",
        "* Alternative Hypothesis: The mean of Total_Links is not equal among the location (A, B, C, D, E, F, G) (H1: at least one mean is different from the others)\n",
        "* Test Type : ANOVA Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# perform ANOVA test\n",
        "f_value, p_value = stats.f_oneway(data[data['Customer_Location'] == 'A']['Total_Images'],\n",
        "                                  data[data['Customer_Location'] == 'B']['Total_Images'],\n",
        "                                  data[data['Customer_Location'] == 'C']['Total_Images'],\n",
        "                                  data[data['Customer_Location'] == 'D']['Total_Images'],\n",
        "                                  data[data['Customer_Location'] == 'E']['Total_Images'],\n",
        "                                  data[data['Customer_Location'] == 'F']['Total_Images'],\n",
        "                                  data[data['Customer_Location'] == 'G']['Total_Images'])\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis - the Customer_Location has a significant impact on the Total_Images in the email\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis - the Customer_Location does not have a significant impact on the Total_Images in the email\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "For this hypothesis, I used ANOVA (Analysis of Variance) test to abtain P-Value because ANOVA is a statistical test that is used to determine whether there is a statistically significant difference in the means of two or more groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "ANOVA test is used to determine if there are significant difference between the means of two or more groups. In this case, we have different locations (A,B,C,D,E,F,G) and we want to determine if there is a significant difference in the means of Total_Links, Total_Images and Total_Past_Communications among these groups.\n",
        "\n",
        "ANOVA is appropriate test for this case because the variables Total_Links, Total_Images and Total_Past_Communications are continuous and we want to compare the means of multiple groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "## Lets go and see the percentage of missing values\n",
        "data.isnull().mean()*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y55hl3DLoOvN"
      },
      "outputs": [],
      "source": [
        "#divide columns on the basis of percentage of missing values.\n",
        "null_percent_col=data.isnull().mean()[data.isnull().mean()>0]\n",
        "null_percentl_col=null_percent_col[null_percent_col.values<0.05]\n",
        "null_percenth_col=data.isnull().mean()[data.isnull().mean()>0.05]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68lLUFs4oOvN"
      },
      "outputs": [],
      "source": [
        "#missing value percentage greater than 0 and less than 5%.\n",
        "null_percent_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HYhAqY_oOvN"
      },
      "outputs": [],
      "source": [
        "#missing value percentage less than 5%.\n",
        "null_percentl_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-d8dSyQoOvO"
      },
      "outputs": [],
      "source": [
        "# missing value percentage greater than 5%.\n",
        "null_percenth_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWTOOvDXoOvO"
      },
      "outputs": [],
      "source": [
        "#summary about columns on the basis of percentage of missing values.\n",
        "print(f\"There are {len(null_percent_col.index)} columns {null_percent_col.index.values}  having null values and  the columns which have less than 5% null values are {null_percentl_col.index.values} \\n and more than 5% null values are {null_percenth_col.index.values}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS2octqJoOvO"
      },
      "source": [
        "when the percentage of missing data in column is high, then we remove that column. Hence I drop the column 'Customer_Location'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_F8_P9BoOvO"
      },
      "outputs": [],
      "source": [
        "data2=data.drop(columns=['Customer_Location'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXM2uigLoOvP"
      },
      "outputs": [],
      "source": [
        "#checking distribution of other null value to find correct way to impute\n",
        "for cat in ['Total_Past_Communications','Total_Links','Total_Images']:\n",
        "    sns.distplot(data2[cat], hist= True);\n",
        "    feature = data2[cat]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=1,label= 'mean');  #red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=1,label='median'); #cyan\n",
        "    plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right')\n",
        "    plt.title(f'{cat.title()}');\n",
        "    plt.xlabel(cat)\n",
        "    plt.show()\n",
        "    print('='*120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w--9oXyqoOvP"
      },
      "source": [
        "Since, all the variables are positively skewed hence we fill median in place of null variable present in the variable because it is not influenced by the outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i1z80XtoOvP"
      },
      "outputs": [],
      "source": [
        "data2[\"Total_Links\"].fillna(data2[\"Total_Links\"].median(),inplace=True)\n",
        "data2[\"Total_Images\"].fillna(data2[\"Total_Images\"].median(),inplace=True)\n",
        "data2[\"Total_Past_Communications\"].fillna(data2[\"Total_Past_Communications\"].median(),inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LSzHDIYoOvP"
      },
      "outputs": [],
      "source": [
        "## Again check the percentage of missing values\n",
        "data2.isnull().sum().mean()*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "Missing value imputation techniques deals with replacing the null value by  measures of central tendency. The Most commonly used measures of central tendency are the mean, median, and mode.\n",
        "\n",
        "I replace the null values with median in all the variable because all the variables are positively skewed and The median is the value in the middle of a dataset i.e., it is not influenced by the outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "# Importing scipy\n",
        "import scipy\n",
        "#Find Skewness in all numerical variable\n",
        "skewness= [{num_col:scipy.stats.skew(data2[num_col])} for num_col in num_feature ]\n",
        "skewness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r6vMqwMoOvR"
      },
      "outputs": [],
      "source": [
        "def find_outliers(data, col):\n",
        "    Q1 = data[col].quantile(0.25)\n",
        "    Q3 = data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
        "\n",
        "    return outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-eC-npZoOvR"
      },
      "outputs": [],
      "source": [
        "for feature in ['Total_Links', 'Total_Images','Subject_Hotness_Score','Total_Past_Communications']:\n",
        "    outliers = find_outliers(data2, feature)\n",
        "    print(f\"Outliers in {feature}: {len(outliers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hmjgv61oOvR"
      },
      "outputs": [],
      "source": [
        "total_outliers = []\n",
        "\n",
        "for feature in num_feature:\n",
        "    outliers = find_outliers(data2, feature)\n",
        "    total_outliers.append(outliers.index)\n",
        "\n",
        "# Combine the outlier indices into a single set\n",
        "outliers_index = set().union(*total_outliers)\n",
        "\n",
        "# Drop rows with outliers\n",
        "data3 = data2.drop(list(outliers_index), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvzKuaEQoOvS"
      },
      "outputs": [],
      "source": [
        "data3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaZGvrxroOvS"
      },
      "outputs": [],
      "source": [
        "#before removal of outliers\n",
        "print(f\"number of rows are {data2.shape[0]} before removal of outliers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AokjwakPoOvS"
      },
      "outputs": [],
      "source": [
        "#after removal of outliers\n",
        "print(f\"number of rows are {data3.shape[0]} before removal of outliers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "Initially, I find Skewness for each numerical variable in order to know about distribution of the variable, hence we got to know that all variables are positively skewed except Word_Count i.e., there are some outliers present in the variable. To remove those outliers, I choose to use trimming technique which excludes the outlier values.\n",
        "\n",
        "I used this technique because it improved the quality of the dataset and enhance the accuracy and stability of statistical models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding\n",
        "---\n",
        "Categorical Encoding is a process where we transform categorical data into numerical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "outputs": [],
      "source": [
        "cat_features= set(cat_feature)-{'Customer_Location'}\n",
        "cat_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCNx6MumoOva"
      },
      "source": [
        "Because `customer_Location` have high percentage of missing value thats why we exclude from the further Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxt_5SWpoOva"
      },
      "outputs": [],
      "source": [
        "data4=pd.get_dummies(data3, columns=cat_features, drop_first=True).reset_index().drop(columns=[\"index\"])\n",
        "data4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwyd4mGqoOvb"
      },
      "outputs": [],
      "source": [
        "data4.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "Here I have used one hot encoding technique on categorial feature for transforming categorical data into numerical, where all the variables are nominal.\n",
        "\n",
        "I used One hot encoding because it makes our data more useful and expressive, and it can be rescaled easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection\n",
        "----\n",
        "We previously saw that there are highly correlated numeric features. `'Total_Links'` and `'Total_Images'`. We can combine these two features to create a new feature and drop the original features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "# creating a new column called Total_link_Lmages\n",
        "data4['Total_link_Images'] = data4['Total_Links'] + data4['Total_Images']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld_asQkgoOvd"
      },
      "outputs": [],
      "source": [
        "# dropping the two original columns\n",
        "data4.drop(columns=['Total_Links','Total_Images'],inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juGcMfAioOvd"
      },
      "outputs": [],
      "source": [
        "#after manipulation\n",
        "data4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4-P-D4moOvd"
      },
      "outputs": [],
      "source": [
        "data4.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "outputs": [],
      "source": [
        "#Independent variables\n",
        "col=set(data4.columns.values)-{\"Email_Status\"}\n",
        "col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V55pXW7VoOve"
      },
      "outputs": [],
      "source": [
        "#Multicollinearity by VIF\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY0okXjsoOve"
      },
      "outputs": [],
      "source": [
        "calc_vif(data4[[i for i in col]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mqBWXQMoOvf"
      },
      "outputs": [],
      "source": [
        "calc_vif(data4[[i for i in col if i not in [\"Email_Campaign_Type_2\"]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_IHP7l6oOvf"
      },
      "outputs": [],
      "source": [
        "new_num_features=calc_vif(data4[[i for i in col if i not in [\"Email_Campaign_Type_2\"]]]).variables.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lohk6FoLoOvf"
      },
      "outputs": [],
      "source": [
        "data4.drop(columns=[\"Email_Campaign_Type_2\"],inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      },
      "source": [
        "I Calculate Variance Inflation Factor (VIF) which help us in the detection of multicollinearity in the data. I found some features having VIF of more than 5-10 and I considered it to be 6 then I dropped the multicolinear feature `\"Email_Campaign_Type_2\"` & `\"Customer_Location_G\"` to make the VIF less than 6.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGgaEstsBnaf"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def randomforest_embedded(x, y):\n",
        "    # Create the random forest with hyperparameters\n",
        "    model = RandomForestClassifier(n_estimators=550)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(x, y)\n",
        "\n",
        "    # Get the importance of the resulting features\n",
        "    importances = model.feature_importances_\n",
        "\n",
        "    # Create a data frame for visualization\n",
        "    final_df = pd.DataFrame({\"Features\": pd.DataFrame(x).columns, \"Importances\": importances})\n",
        "    final_df.set_index('Importances')\n",
        "\n",
        "    # Sort in ascending order for better visualization\n",
        "    final_df = final_df.sort_values('Importances')\n",
        "\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz2mk3S1oOvi"
      },
      "outputs": [],
      "source": [
        "# Getting feature importance of selected features\n",
        "randomforest_embedded(x=data4[new_num_features],y=data4[\"Email_Status\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlTW0f0foOvi"
      },
      "outputs": [],
      "source": [
        "#Drop the columns which are insignificant for our dataset.\n",
        "drop=['Time_Email_sent_Category_3',\"Email_Type_2\"]\n",
        "data4.drop(drop,inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "# Transform Your data\n",
        "# Getting symmetric and skew symmetric features from the columns\n",
        "symmetric_feature = []\n",
        "non_symmetric_feature = []\n",
        "\n",
        "# Looping through columns in the dataset\n",
        "for i in data4.describe().columns.values:\n",
        "    # Check if the absolute difference between mean and median is less than 0.1\n",
        "    if abs(data4[i].mean() - data4[i].median()) < 0.1:\n",
        "        symmetric_feature.append(i)\n",
        "    else:\n",
        "        non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\", symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\", non_symmetric_feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPklQN0HoOvk"
      },
      "source": [
        "Since, all features have skew symmetric distribution, hence our data needs to be transforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6RmzZ9ZoOvk"
      },
      "source": [
        "Primarily I plot the probability plot which is a graphical technique for assessing whether or not a dataset follows a normal distribution. The data are plotted against a theoretical distribution in such a way that the points should form approximately a straight line.\n",
        "\n",
        "if the points form approximately a straight line then, we said that dataset follows a normal distribution. otherwise, it does not follow a normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Jc66wEoOvl"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import *\n",
        "from scipy import stats\n",
        "\n",
        "# Loop through non-symmetric features\n",
        "for variable in non_symmetric_feature:\n",
        "    sns.set_context('notebook')\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Subplot 1: Histogram\n",
        "    plt.subplot(1, 2, 1)   # means 1 row, 2 Columns and 1st plot\n",
        "    data4[variable].hist(bins=30)\n",
        "\n",
        "    # Subplot 2: QQ plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    stats.probplot(data4[variable], dist='norm', plot=plt)\n",
        "    plt.title(variable)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Print separator line\n",
        "    print('=' * 120)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKa4fXu2oOvl"
      },
      "outputs": [],
      "source": [
        "## since categorial feature does not required transformation hence take continuous feature\n",
        "for col in ['Total_Past_Communications', 'Word_Count', 'Total_link_Images', 'Subject_Hotness_Score']:\n",
        "    # Doing square root transformation\n",
        "    data4[col] = np.sqrt(data4[col])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRN-xT_6oOvm"
      },
      "source": [
        "I used square-root transformation in all continuous features because all are moderately skew."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N798uunoOvm"
      },
      "outputs": [],
      "source": [
        "#draw the distribution plot of transform features with the value of mean and median.\n",
        "for i,col in enumerate(['Subject_Hotness_Score','Total_Past_Communications','Word_Count',\"Total_link_Images\"]) :\n",
        "    plt.figure(figsize = (18,18))\n",
        "    plt.subplot(6,2,i+1);\n",
        "    sns.distplot(data4[col], color = '#055E85', fit = norm);\n",
        "    feature = data4[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n",
        "    plt.title(f'{col.title()}');\n",
        "    plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling\n",
        "---\n",
        "If the ranges of features have large difference then we should use feature scaling which help us in getting all the features in similar range.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEWmHcefoOvn"
      },
      "outputs": [],
      "source": [
        "data4.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "#standard scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "for col in ['Subject_Hotness_Score','Total_Past_Communications','Word_Count',\n",
        "            'Total_link_Images']:\n",
        "    data4[col] = StandardScaler().fit_transform(data4[col].values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjkFSQ1VoOvr"
      },
      "outputs": [],
      "source": [
        "data4.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6saF1KCoOvr"
      },
      "source": [
        "Basically,\n",
        " we use Standardization when your data follows Gaussian distribution and use Normalization when your data does not follow Gaussian distribution.\n",
        "\n",
        "Since all of numerical features are almost normal distributed, hence I applied standard scaler to scale the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      },
      "source": [
        "This dataset doesnot need any dimensionality reduction.\n",
        "\n",
        "Dimensionality reduction is a technique that is used to reduce the number of features in a dataset. It is often used when the number of features is very large, as this can lead to problems such as overfitting and slow computation. There are a variety of techniques that can be used for dimensionality reduction, such as principal component analysis (PCA) and singular value decomposition (SVD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data into train and test.\n",
        "X_train, X_test, y_train, y_test = train_test_split(data4.drop(\"Email_Status\",axis=1),data4[\"Email_Status\"],test_size = 0.20, random_state = 0)\n",
        "print(f\"There are {y_train.shape[0]} rows for training and {y_test.shape[0]} for testing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND7WYS-poOvt"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8wt6gEJoOvt"
      },
      "outputs": [],
      "source": [
        "data4['Email_Status'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "Imbalance means that the number of data points available for different the classes is different: If there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.\n",
        "\n",
        "In our dataset, dependent column data ratio is 80:16:4. So, during model creating it's obvious that there will be bias and having a great chance of predicting the majority one so frequently. So the dataset should be balanced before it going for the model creation part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "# Handaling imbalance dataset using SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train, y_train = sm.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY7m4YYGoOvv"
      },
      "outputs": [],
      "source": [
        "#visualization of resampled data\n",
        "from collections import Counter\n",
        "counter = Counter(y_train)\n",
        "for key,value in counter.items():\n",
        "    per = value / len(y_train) * 100\n",
        "    print('Class=%d, n=%d (%.3f%%)' % (key, value, per))\n",
        "# plot the distribution\n",
        "plt.bar(counter.keys(), counter.values())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zepu9FxXoOvw"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix,f1_score, recall_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve,roc_auc_score,precision_score, roc_auc_score\n",
        "# from sklearn.metrics import roc_auc_ovr\n",
        "# from sklearn.metrics import roc_auc_ovo\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import XGBRFClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "lr = LogisticRegression(fit_intercept=True,\n",
        "            class_weight='balanced',multi_class='multinomial')\n",
        "# Fit the Algorithm\n",
        "lr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy3-riF_oOvx"
      },
      "outputs": [],
      "source": [
        "# Checking the coefficients\n",
        "lr.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faJIkHS4oOvx"
      },
      "outputs": [],
      "source": [
        "# Checking the intercept value\n",
        "lr.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7MlcN9CoOvy"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "train_probability_lr = lr.predict_proba(X_train)\n",
        "test_probability_lr = lr.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtspnRcoOvy"
      },
      "outputs": [],
      "source": [
        "# Get the predicted classes\n",
        "y_pred_train_lr = lr.predict(X_train)\n",
        "y_pred_lr = lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGi3w29foOvy"
      },
      "outputs": [],
      "source": [
        "#define a function which print the result of Evaluation metrics.\n",
        "def print_metrics(actual_train,actual_test,predicted_train,predicted_test,test_probability):\n",
        "\n",
        "    print('accuracy on train data is {}'.format(accuracy_score(actual_train,predicted_train)))\n",
        "    print('accuracy on test data is {}'.format(accuracy_score(actual_test,predicted_test)))\n",
        "    print('precision on test data is {}'.format(precision_score(actual_test,predicted_test,average='weighted')))\n",
        "    print('recall on test data is {}'.format(recall_score(actual_test,predicted_test,average='weighted')))\n",
        "    print('f1 Score on test data is {}'.format(f1_score(actual_test,predicted_test,average='weighted')))\n",
        "    print('roc_auc_score on test data is {}'.format(roc_auc_score(actual_test, test_probability,multi_class='ovr',average='weighted')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "print_metrics(y_train,y_test,y_pred_train_lr,y_pred_lr,test_probability_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJZ2GIgToOvz"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart that is confusion matrix for both training and testing data\n",
        "\n",
        "def confusion_metrics(actual_train,actual_test,predicted_train,predicted_test):\n",
        "    labels = ['Ignored', 'Opened', 'Acknowledged']\n",
        "    cm1 = confusion_matrix(actual_train, predicted_train)\n",
        "    ax1= plt.subplot()\n",
        "    sns.heatmap(cm1, annot=True, ax = ax1) #annot=True to annotate cells\n",
        "    ax1.set_xlabel('Predicted labels')\n",
        "    ax1.set_ylabel('True labels')\n",
        "    ax1.set_title('Confusion Matrix for training data')\n",
        "    ax1.xaxis.set_ticklabels(labels)\n",
        "    ax1.yaxis.set_ticklabels(labels)\n",
        "    plt.show()\n",
        "    print(\" \")\n",
        "\n",
        "    cm2 = confusion_matrix(actual_test, predicted_test)\n",
        "    ax2= plt.subplot()\n",
        "    sns.heatmap(cm2, annot=True, ax = ax2)\n",
        "  # labels, title and ticks\n",
        "    ax2.set_xlabel('Predicted labels')\n",
        "    ax2.set_ylabel('True labels')\n",
        "    ax2.set_title('Confusion Matrix for testing data')\n",
        "    ax2.xaxis.set_ticklabels(labels)\n",
        "    ax2.yaxis.set_ticklabels(labels)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBlWKhEdoOvz"
      },
      "outputs": [],
      "source": [
        "confusion_metrics(y_train,y_test,y_pred_train_lr,y_pred_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques.\n",
        "model = LogisticRegression(fit_intercept=True, max_iter=10000,\n",
        "            class_weight='balanced',multi_class='multinomial')\n",
        "solvers = ['lbfgs']\n",
        "penalty = ['l2']\n",
        "c_values = [1000,100, 10, 1.0, 0.1, 0.01,0.001]\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_result=grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDAEyDb2oOv0"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "train_probability_lrh = grid_result.predict_proba(X_train)\n",
        "test_probability_lrh = grid_result.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioEQuRx7oOv1"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Get the predicted classes\n",
        "y_pred_train_lrh = grid_result.predict(X_train)\n",
        "y_pred_lrh = grid_result.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "outputs": [],
      "source": [
        "print_metrics(y_train,y_test,y_pred_train_lrh,y_pred_lrh,test_probability_lrh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIEbSACRoOv2"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart that is confusion matrix for both training and testing data\n",
        "confusion_metrics(y_train,y_test,y_pred_train_lrh,y_pred_lrh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl8LxflboOv2"
      },
      "source": [
        "There is no such improvement in the result while using Cross- Validation & Hyperparameter Tuning because our dataset is large enough to give accurate result using Hold-Out Method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2 Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofobAd77oOv2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 2 Implementation\n",
        "# Create an instance of the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_model.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKQ7qSRwoOv3"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "y_pred_train_rf = rf_model.predict(X_train)\n",
        "y_pred_rf = rf_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16N2XeGkoOv3"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "train_probability_rf = rf_model.predict_proba(X_train)\n",
        "test_probability_rf = rf_model.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "print_metrics(y_train,y_test,y_pred_train_rf,y_pred_rf,test_probability_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79JBT1BPoOv4"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart that is confusion matrix for both training and testing data\n",
        "confusion_metrics(y_train,y_test,y_pred_train_rf,y_pred_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Grid search\n",
        "rf_grid = GridSearchCV(estimator=rf_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2, scoring='f1')\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldc7BlRfoOv4"
      },
      "outputs": [],
      "source": [
        "#best parameter\n",
        "print(\"Best: %f using %s\" % (rf_grid.best_score_, rf_grid.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMI6X7lkoOv5"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "y_pred_train_rfh = rf_grid.predict(X_train)\n",
        "y_pred_rfh = rf_grid.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWHzTGgHoOv5"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "train_probability_rfh = rf_grid.predict_proba(X_train)\n",
        "test_probability_rfh = rf_grid.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "outputs": [],
      "source": [
        "print_metrics(y_train,y_test,y_pred_train_rfh,y_pred_rfh,test_probability_rfh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF85yMkaoOv5"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart that is confusion matrix for both training and testing data\n",
        "confusion_metrics(y_train,y_test,y_pred_train_rf,y_pred_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "I have used these metrices for evaluation of the model and their impact on business are as follows:\n",
        "\n",
        "**Accuracy:** This metric indicates the percentage of correctly classified instances out of the total number of instances. In a business setting, this would indicate the overall effectiveness of the model in making correct predictions. A high accuracy score would have a positive impact on the business, as it would indicate a high level of confidence in the model's predictions.\n",
        "\n",
        "**Precision:** This metric indicates the proportion of true positive predictions out of all positive predictions made by the model. In a business setting, this would indicate the level of confidence in the model's ability to identify positive instances correctly. A high precision score would have a positive impact on the business, as it would indicate that the model is not making false positive predictions.\n",
        "\n",
        "**Recall:** This metric indicates the proportion of true positive predictions out of all actual positive instances. In a business setting, this would indicate the model's ability to identify all positive instances. A high recall score would have a positive impact on the business, as it would indicate that the model is not missing any positive instances.\n",
        "\n",
        "**F1 Score:** This metric is a combination of precision and recall and is used to balance the trade-off between the two. In a business setting, this would indicate the overall effectiveness of the model in making correct predictions while also avoiding false positives and false negatives. A high F1 score would have a positive impact on the business, as it would indicate that the model is making accurate predictions while also being able to identify all positive instances.\n",
        "\n",
        "**ROC AUC:** This metric indicates the ability of the model to distinguish between positive and negative instances. In a business setting, this would indicate the model's ability to correctly classify instances as positive or negative. A high ROC AUC score would have a positive impact on the business, as it would indicate that the model is able to correctly classify instances.\n",
        "\n",
        "In summary, the Random Forest Classifier can be considered as an efficient model for the business, especially when it achieves high scores in all of these evaluation metrics, which would indicate that it can accurately predict outcomes, identify all positive instances, and correctly classify instances as positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 3 XgBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the RandomForestClassifier\n",
        "xg_model = XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "xg_models=xg_model.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG921tjqoOv7"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_xg = xg_models.predict(X_train)\n",
        "y_pred_xg = xg_models.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxqwMwJVoOv7"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "train_probability_xg = xg_models.predict_proba(X_train)\n",
        "test_probability_xg = xg_models.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1z2sKpx6M"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAvl7lEioOv7"
      },
      "outputs": [],
      "source": [
        "print_metrics(y_train,y_test,y_pred_train_xg,y_pred_xg,test_probability_xg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart that is confusion matrix for both training and testing data\n",
        "confusion_metrics(y_train,y_test,y_pred_train_xg,y_pred_xg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PIHJqyupx6M"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# Hyperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Create an instance of the XGBClassifier\n",
        "xg_model = XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Grid search\n",
        "xg_grid = GridSearchCV(estimator=xg_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2, scoring='roc_auc')\n",
        "\n",
        "xg_grid1=xg_grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKq5ZZhWoOwA"
      },
      "outputs": [],
      "source": [
        "#best parameter\n",
        "print(\"Best: %f using %s\" % (xg_grid.best_score_, xg_grid.best_params_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdMAJ0lqoOwA"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_xgh = xg_grid1.predict(X_train)\n",
        "y_pred_xgh = xg_grid1.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQSdmgAloOwA"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "train_probability_xgh = xg_grid1.predict_proba(X_train)\n",
        "test_probability_xgh = xg_grid1.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qAgymDpx6N"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMffxkwpx6N"
      },
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hykwinpx6N"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzVzZC6opx6N"
      },
      "outputs": [],
      "source": [
        "print_metrics(y_train,y_test,y_pred_train_xgh,y_pred_xgh,test_probability_xgh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lSw50FVoOwB"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart that is confusion matrix for both training and testing data\n",
        "confusion_metrics(y_train,y_test,y_pred_train_xgh,y_pred_xgh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "When evaluating the effectiveness of an email campaign in a classification model, the following evaluation metrics would be considered for a positive business impact:\n",
        "\n",
        "* **Precision**: This metric indicates the proportion of true positive predictions (emails that were opened and resulted in a desired action) out of all positive predictions made by the model. In a business setting, this would indicate the level of confidence in the model's ability to identify individuals who are likely to engage with the campaign. A high precision score would have a positive impact on the business, as it would indicate that the model is not making false positive predictions and is effectively identifying individuals who are likely to engage with the campaign.\n",
        "\n",
        "* **Recall**: This metric indicates the proportion of true positive predictions (emails that were opened and resulted in a desired action) out of all actual positive instances (emails that were opened and resulted in a desired action). In a business setting, this would indicate the model's ability to identify all individuals who engaged with the campaign. A high recall score would have a positive impact on the business, as it would indicate that the model is not missing any individuals who engaged with the campaign.\n",
        "\n",
        "* **F1 Score**: This metric is a combination of precision and recall and is used to balance the trade-off between the two. In a business setting, this would indicate the overall effectiveness of the model in identifying individuals who are likely to engage with the campaign while also avoiding false positives and false negatives. A high F1 score would have a positive impact on the business, as it would indicate that the model is effectively identifying individuals who are likely to engage with the campaign while also being able to identify all individuals who engaged with the campaign.\n",
        "\n",
        "* **ROC AUC**: This metric indicates the ability of the model to distinguish between positive and negative instances. In a business setting, this would indicate the model's ability to correctly classify instances as positive (engaged with the campaign) or negative (did not engage with the campaign). A high ROC AUC score would have a positive impact on the business, as it would indicate that the model is able to correctly classify individuals as likely to engage with the campaign or not.\n",
        "\n",
        "The evaluation metrics that would be considered for a positive business impact of an email campaign effectiveness in a classification model are **precision, recall** which combine to provide F1 score. These metrics would indicate the model's ability to identify individuals who are likely to engage with the campaign while also being able to identify all individuals who engaged with the campaign, and correctly classify instances as positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 2. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lf4sZzyoOwD"
      },
      "outputs": [],
      "source": [
        "#all classifiers\n",
        "Model = [\"Logistic Regression\",\"Random Forest\",\"Xgboost\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Syl2WX2MoOwD"
      },
      "outputs": [],
      "source": [
        "#creating dataframe for all classifiers using dictionary\n",
        "#creating dataframe for all classifiers using dictionary\n",
        "pd.DataFrame({\"Model\":Model,\n",
        "'Train Accuracy':[0.502620,0.543123,0.697187],\n",
        "'Test Accuracy':[0.603999,0.654914,0.732894],\n",
        "'Precision':[0.777709,0.783135,0.780748],\n",
        "'Recall':[0.603999,0.654914,0.732894],\n",
        "'F1 Score':[0.670009,0.703793,0.752840],\n",
        "'roc_auc_score':[0.726082,0.7654076,0.784439]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ai3vooSoOwD"
      },
      "source": [
        "We can see from above table that\n",
        "\n",
        "1) Xgboost have highest Training and Testing Accuracy.\n",
        "\n",
        "2) Xgboost also have best Recall score, F1 Score and Roc_auc_Score.\n",
        "\n",
        "Hence we can say that Xgboost is the best Model. Thus, I have choosen XGBoost model which is hyperparameter optimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gk_QB43oOwE"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKjOQipaoOwE"
      },
      "outputs": [],
      "source": [
        "#Feature Importance\n",
        "feature_importances_xg = pd.DataFrame(xg_models.feature_importances_,\n",
        "                                   index = X_train.columns,\n",
        "                                    columns=['importance_xg']).sort_values('importance_xg',\n",
        "                                                                        ascending=False)[:10]\n",
        "\n",
        "plt.subplots(figsize=(17,6))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(feature_importances_xg.index, feature_importances_xg['importance_xg'],\n",
        "        color=\"green\",  align=\"center\")\n",
        "plt.xticks(feature_importances_xg.index, rotation = 85)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "* ### As we observed that Email_Campaign_Type_3 was the most important feature. If your Email_Campaign_Type was 1, there is a 90% likelihood of your Email to be read/acknowledged.\n",
        "\n",
        "\n",
        "* ### It was observed that both Time_Email_Sent and Customer_Location was insignificant in determining the Email status. The ratio of the Email Status was the same irrespective of the demographic location or the time frame the emails were sent on.\n",
        "\n",
        "  ### Emails sent in category 2 during the middle of the day will undoubtedly receive more reading and acknowledgment than those sent in categories 1 and 2 during the day.\n",
        "\n",
        "* ### As the word_count increases beyond the 600 mark we see that there is a high possibility of that email being ignored. The ideal mark is 400-600. No one is interested in reading long emails!\n",
        "\n",
        "* ### Emails that were ignored contained more pictures.\n",
        "\n",
        "* ### With the exception of Word Count, practically all continuous variables had outliers. After analysis, it was discovered that outliers account for more than 5% of the minority data and will affect the results in either direction, therefore it was preferable to leave them in.\n",
        "\n",
        "* ### Although SMOTE appears to have performed much better, information loss is possible.\n",
        "\n",
        "* ### Based on the metrics, XGBoost Classifier worked the best, giving a test score of 76% for F1 score and 78% for roc-auc score respectively.\n",
        "\n",
        "----\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coOssAfnoOwE"
      },
      "source": [
        "`Thank-You  :-)  `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzp1n7duoOwE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ijmpgYnKYklI",
        "4Of9eVA-YrdM",
        "Yfr_Vlr8HBkt"
      ],
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}